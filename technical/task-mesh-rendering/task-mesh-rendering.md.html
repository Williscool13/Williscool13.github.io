**_Task and Mesh Shaders: A Practical Guide (Slang)_**
    [William Gunawan](https://www.williscool.com)
    Written on 2025/12/04

# Introduction

Mesh shaders represent a fundamental shift in GPU rendering pipelines.
Unlike traditional vertex shaders that process vertices individually, mesh shaders adopt a compute-like programming model with explicit thread dispatch and shared memory access.

This article will demonstrate a practical task/mesh shader implementation in Vulkan with Slang, including:
- Trivial Task + Mesh shader implementation (Sascha Willems' Sample)
- Basic mesh shader pipeline
- Task + Mesh shader with frustum and backface culling

For a comprehensive explanation of the mesh shader model, refer to NVIDIA's [Introduction to Mesh Shaders](https://developer.nvidia.com/blog/introduction-turing-mesh-shaders/) and AMD's [Mesh Shader Guide](https://gpuopen.com/learn/mesh_shaders/mesh_shaders-from_vertex_shader_to_mesh_shader/).

For performance comparisons and detailed profiling results, see my [task/mesh shader benchmark](../../technical/task-mesh-benchmarking/task-mesh-benchmarking.md.html), which demonstrates improved cache hit rates and finer culling granularity compared to traditional pipelines.

I assume familiarity with Vulkan basics: pipeline creation, descriptors, and buffer device addresses.
Throughout this article, I access buffer data exclusively through BDAs without using samplers or textures.

Source code available at https://github.com/Williscool13/TaskMeshRendering.

## Terminology

**Task Shader / Amplification Shader**

An optional pre-processing stage that determines which mesh shader workgroups to spawn. Performs coarse culling (e.g., per-meshlet frustum culling) before mesh shading.
Typically dispatched with 32-128 threads per workgroup to evaluate multiple meshlets in parallel.
While all threads can `DispatchMesh` for mesh shader workgroups, only one needs to do it after a group shared sync.
Called "Amplification Shader" in DirectX 12.

**Mesh Shader**

Generates primitives and vertices for rasterization.
Replaces the traditional vertex/geometry shader stages.
Outputs a variable number of triangles per workgroup (up to hardware limits, varies by vendor but typically 256 vertices/256 triangles).
Though mesh shaders are not limited to triangles (you can output other primitives), triangles will be the focus of this article.

**Meshlet**

A small cluster of vertices and triangles, typically 32-64 vertices and 64-124 triangles. See why in the tips section of this [article](https://developer.nvidia.com/blog/using-mesh-shaders-for-professional-graphics/).
Meshlets are the atomic unit processed by mesh shaders, designed to fit within GPU shared memory and optimize cache locality.

**Thread / Invocation**

A single execution instance within a thread group.
Threads within a thread group can cooperate via shared memory and barriers.

**Thread Group / Workgroup**

A collection of threads dispatched together, sharing local memory and synchronization primitives.
In task shaders, one workgroup typically evaluates multiple meshlets (often one per thread) and emits mesh shader workgroups for visible ones.
In mesh shaders, one workgroup processes exactly one meshlet.

**Cone Culling / Meshlet Backface Culling**

Conservative culling of meshlets whose cone normal indicates all contained triangles face away from the camera.
Not to be confused with traditional per-triangle backface culling in the rasterizer, this operates at meshlet granularity in the task shaders to avoid processing invisible geometry entirely.


# Basic Task and Mesh Shader (Sascha Willems' Sample)

We'll start with a basic task/mesh shader adapted from Sascha Willems' sample repository. This was where I first looked when implementing task/mesh shader in Vulkan.
While it demonstrates minimum viable implementation, it lacks enough detail to truly understand task/mesh shader capabilities.

I've adapted the shader a bit to fit my own code, but the general idea is still the same:

``````````````````````
// Slang
import common;

static const float4 positions[3] = {
    float4( 0.0, -1.0, 0.0, 1.0),
    float4(-1.0,  1.0, 0.0, 1.0),
    float4( 1.0,  1.0, 0.0, 1.0)
};


struct VertexOutput
{
    float4 position : SV_Position;
    float4 color : TEXCOORD0;
};

struct DummyPayLoad
{
    uint dummyData;
};


struct PushConstant {
    float4x4 modelMatrix;
    SceneData* sceneData;
};

[shader("task")]
[numthreads(1, 1, 1)]
void taskMain()
{
    DummyPayLoad localPayload;
    DispatchMesh(3, 1, 1, localPayload);
}


[shader("mesh")]
[outputtopology("triangle")]
[numthreads(1, 1, 1)]
void meshMain(out vertices VertexOutput vertices[3], out indices uint3 triangles[1],
    uint3 DispatchThreadID : SV_DispatchThreadID, uint3 groupId : SV_GroupID,
    uniform PushConstant pushConstant)
{
    float4x4 mvp = mul(pushConstant.sceneData.viewProj, pushConstant.modelMatrix);
    uint meshletID = groupId.x;

    // Hash-based per-meshlet coloring for visualization
    uint hash = meshletID * 747796405u + 2891336453u;
    float3 color = float3(
        (hash & 0xFF) / 255.0,
        ((hash >> 8) & 0xFF) / 255.0,
        ((hash >> 16) & 0xFF) / 255.0
    );

    float4 offset = float4(0.0, 0.0, float(DispatchThreadID.x), 0.0);

    SetMeshOutputCounts(3, 1);
    for (uint i = 0; i < 3; i++) {
        vertices[i].position = mul(mvp, positions[i] + offset);
        vertices[i].color = float4(color, 1.0f);
    }

    triangles[0] = uint3(0, 1, 2);
}

[shader("fragment")]
float4 fragmentMain(VertexOutput input)
{
    return input.color;
}
``````````````````````

Let's take a look at the CPU code before further exploring the shader code.
This example is meant to be called from the CPU with:
``````````````````````
vkCmdDrawMeshTasksEXT(cmd, 1, 1, 1);
``````````````````````
Fairly simple, we are dispatching a single task shader group.
The pipeline flow:

Task Shader -> Mesh Shader -> Fragment Shader

The dispatched task shader group is hardcoded to spawn 3 mesh shader groups, each processing one "meshlet" (in this case, a single triangle; real meshlets typically contain 32-64 vertices).
However, this example is so trivial you could skip the task shader entirely and just call `vkCmdDrawMeshTasksEXT(cmd, 3, 1, 1)` directly (create the pipeline without a task shader).

![Trivial Task/Mesh Shader](3Triangles.png)

This sample is a good start to ensure that task and mesh shaders actually work in your renderer. It fills in a few basic questions like:
- Does my device support Task/Mesh shaders?
- Have I set up my pipeline to draw to my screen correctly?
- Does my shader compilation work correctly for task and mesh shaders?

It also answers some basic questions about how task and mesh shaders work together.

**Task Shader**

- Dispatches N mesh shader groups.
- Only one thread should call DispatchMesh (typically thread 0 after a barrier). Or, all threads can call it with identical parameters (hardware will deduplicate)
- The dispatch must include a payload, which should contain information for the mesh shader.

The task shader dispatches the mesh shader groups with the command `DispatchMesh(x, y ,z, Payload p)`, which takes 4 parameters.
X, Y, Z dispatch should be familiar to you as the standard compute-shader dispatch style.

The payload communicates data from task to mesh shader. While syntactically required (and empty in this example), real applications typically use it to pass meshlet indices.
Since task shader threads may cooperatively populate the payload, it requires groupshared synchronization before dispatch.

Payload size significantly impacts performance. [NVIDIA recommends](https://developer.nvidia.com/blog/using-mesh-shaders-for-professional-graphics/) keeping task shader outputs below 236 or 108 bytes, preferring compact data types (uint8_t, uint16_t) when possible.
In my testing, reducing payload size from ~260+ to ~86 bytes improved draw call time from ~0.09ms to ~0.06ms according to GPU profiler measurements, a 33% speedup.

As mentioned above, because of how trivial this example is, the task shader serves no real purpose. If the payload is unused, it is almost always better to just skip the task shader and directly dispatch the mesh shader.

**Mesh Shader**

- Each mesh shader group independently constructs a mini-vertex and mini-index buffer to be rasterized and sent to the fragment shader.
- The mesh shader should indicate how many triangles and indices it intends on outputting. This is done with `SetMeshOutputCounts(vertices, triangles)`, and all threads in the mesh shader group must call SetMeshOutputCounts with identical values.


Just to emphasize, each mesh shader group operates on 1 meshlet. This example uses 1 thread per group for simplicity, but production code typically uses 32-128 threads per task group and 32-128 threads per mesh group.
So if a mesh shader has groupsize (32, 1, 1), each of those threads should help populate a single shared mini-vertex and mini-index buffer, that will be used to draw a single meshlet.

![Single-Meshlet Processing in Task/Mesh Pipeline](SampleShaderFlow.png)

For production use, we need:
- Task shader performing actual work (culling, payload construction)
- Clear payload usage patterns
- Mesh shader demonstrating thread cooperation

The next section will explore these points.

# Data Preparation

Mesh shaders operate on meshlets rather than individual vertices. We use [meshoptimizer](https://github.com/zeux/meshoptimizer) to partition our mesh and generate the required data structures.
For deeper explanation of meshlet theory and generation algorithms, see meshoptimizer's documentation.

(##) Meshlet Generation

The core function is `meshopt_buildMeshlets`, which takes vertex and index buffers and outputs three separate buffers:

- **Meshlet Vertices** (uint32_t) - Indices into the vertex buffer
- **Meshlet Triangles** (uint8_t) - Indices into meshlet vertices
- **Meshlets** (meshopt_Meshlet) - Meshlet descriptors with culling metadata

Note the indirection: `meshletTriangles` contains indices into `meshletVertices`, which contains indices into the original vertex buffer.

When building meshlets, we specify maximum vertices and triangles per meshlet.
I use 64 vertices and 64 triangles. [NVIDIA recommends](https://developer.nvidia.com/blog/using-mesh-shaders-for-professional-graphics/) 64 vertices with 84-124 primitives for optimal alignment, though these values depend on vertex attributes and shader outputs.

``````````````````````
// c++
const size_t maxVertices = 64;
const size_t maxTriangles = 64;

size_t max_meshlets = meshopt_buildMeshletsBound(primitiveIndices.size(), maxVertices, maxTriangles);
std::vector<\meshopt_Meshlet> meshlets(max_meshlets);
std::vector<\unsigned int> meshletVertices(primitiveIndices.size());
std::vector<\unsigned char> meshletTriangles(primitiveIndices.size());

std::vector<\uint32_t> primitiveVertexPositions;
meshlets.resize(meshopt_buildMeshlets(&meshlets[0], &meshletVertices[0], &meshletTriangles[0],
                                      primitiveIndices.data(), primitiveIndices.size(),
                                      reinterpret_cast<\const float*>(primitiveVertices.data()), primitiveVertices.size(), sizeof(Vertex),
                                      maxVertices, maxTriangles, 0.f));

``````````````````````
For the Stanford Bunny (69,451 vertices), this produces 2,251 meshlets. Individual meshlets vary in size, not all contain the maximum 64 vertices or triangles.

![Buffer indirection: meshlet triangles index into meshlet vertices, which index into the vertex buffer](MeshletGeneration.png)


Before generating culling metadata, we optimize each meshlet for GPU cache locality:

``````````````````````
// c++
// Reorder vertices and triangles within each meshlet for optimal GPU vertex cache utilization
for (auto& meshlet : meshlets) {
    meshopt_optimizeMeshlet(&meshletVertices[meshlet.vertex_offset], &meshletTriangles[meshlet.triangle_offset], meshlet.triangle_count, meshlet.vertex_count);
}
``````````````````````

Finally, we generate culling metadata and package everything into our GPU-friendly struct. Meshoptimizer's `meshopt_computeMeshletBounds` provides the bounding sphere and cone data needed for culling:

``````````````````````
// c++
struct Meshlet
{
    glm::vec4 meshletBoundingSphere;

    glm::vec3 coneApex;
    float coneCutoff;

    glm::vec3 coneAxis;
    uint32_t vertexOffset;

    uint32_t meshletVerticesOffset;
    uint32_t meshletTriangleOffset;
    uint32_t meshletVerticesCount;
    uint32_t meshletTriangleCount;
    };

// Generate bounds and extract into my meshlet data structure
for (meshopt_Meshlet& meshlet : meshlets) {
    meshopt_Bounds bounds = meshopt_computeMeshletBounds(
        &meshletVertices[meshlet.vertex_offset],
        &meshletTriangles[meshlet.triangle_offset],
        meshlet.triangle_count,
        reinterpret_cast<\const float*>(primitiveVertices.data()),
        primitiveVertices.size(),
        sizeof(Vertex)
    );

    meshletModel.meshlets.push_back({
        .meshletBoundingSphere = glm::vec4(bounds.center[0], bounds.center[1], bounds.center[2],bounds.radius), // Frustum/Backface Culling
        .coneApex = glm::vec3(bounds.cone_apex[0], bounds.cone_apex[1], bounds.cone_apex[2]), // Backface Culling
        .coneCutoff = bounds.cone_cutoff, // Backface Culling

        .coneAxis = glm::vec3(bounds.cone_axis[0], bounds.cone_axis[1], bounds.cone_axis[2]), // Backface Culling
        .vertexOffset = vertexOffset,

        .meshletVerticesOffset = meshletVertexOffset + meshlet.vertex_offset,
        .meshletTriangleOffset = meshletTrianglesOffset + meshlet.triangle_offset,
        .meshletVerticesCount = meshlet.vertex_count,
        .meshletTriangleCount = meshlet.triangle_count,
    });

}
``````````````````````

The use of culling properties will be discussed later when culling is implemented in the task shader.

# Basic Mesh Shader Pipeline

If a task shader performs no culling or meaningful preprocessing, it adds unnecessary overhead. This section demonstrates mesh shaders in isolation, we'll add task shader culling in the next section.

Our mesh shader accesses all meshlet data through buffer device addresses in push constants:

````````````````````````
struct PushConstant {
    float4x4 modelMatrix;

    SceneData* sceneData;
    VertexData* vertexBuffer;
    MeshletVerticesData* meshletVerticesBuffer;
    MeshletTrianglesData* meshletTrianglesBuffer;
    MeshletData* meshletBuffer;
};
````````````````````````

A mesh shader *group* (not individual thread) processes one meshlet and outputs its vertices/triangles.
Each thread within the group cooperates to populate the output buffers. In this example without task shaders, we use `SV_GroupID.x` to determine which meshlet to process.

````````````````````````
const static int32_t MESH_SHADER_DISPATCH_X = 32;
const static uint MAX_VERTICES = 64;
const static uint MAX_PRIMITIVES = 64;

[shader("mesh")]
[outputtopology("triangle")]
[numthreads(MESH_SHADER_DISPATCH_X, 1, 1)]
void meshMain(
    out indices uint3 triangles[MAX_PRIMITIVES], out vertices VertexOutput vertices[MAX_VERTICES],
    uint3 groupId : SV_GroupID, uint3 gtid : SV_GroupThreadID,
    uniform PushConstant pushConstant)
{
    uint meshletIdx = groupId.x;

    Meshlet meshlet = pushConstant.meshletBuffer->meshlets[meshletIdx];
    float4x4 viewProj = pushConstant.sceneData->viewProj;

    uint hash = meshletIdx * 747796405u + 2891336453u;
    float3 color = float3(
        (hash & 0xFF) / 255.0,
        ((hash >> 8) & 0xFF) / 255.0,
        ((hash >> 16) & 0xFF) / 255.0
    );

    SetMeshOutputCounts(meshlet.meshletVerticesCount, meshlet.meshletTriangleCount);
    uint32_t instanceIndex = gtid.x;
    for (uint i = instanceIndex; i < meshlet.meshletVerticesCount; i += MESH_SHADER_DISPATCH_X) {
        uint localVertexIndex = pushConstant.meshletVerticesBuffer->meshletVertices[meshlet.meshletVerticesOffset + i].vertexIndex;
        Vertex v = pushConstant.vertexBuffer->vertices[meshlet.vertexOffset + localVertexIndex];

        float4 worldPos = mul(pushConstant.modelMatrix, float4(v.position, 1.0));
        float4 clipPos = mul(viewProj, worldPos);

        vertices[i].position = clipPos;
        vertices[i].color = float4(color, 1.0);
    }

    for (uint i = instanceIndex; i < meshlet.meshletTriangleCount; i += MESH_SHADER_DISPATCH_X) {
        uint triOffset = meshlet.meshletTriangleOffset + i * 3;

        uint idx0 = pushConstant.meshletTrianglesBuffer->meshletTriangles[triOffset + 0].triangleIndex;
        uint idx1 = pushConstant.meshletTrianglesBuffer->meshletTriangles[triOffset + 1].triangleIndex;
        uint idx2 = pushConstant.meshletTrianglesBuffer->meshletTriangles[triOffset + 2].triangleIndex;

        triangles[i] = uint3(idx0, idx1, idx2);
    }
}
````````````````````````
The fragment shader simply outputs the interpolated color (omitted for brevity).

In the shader code, each thread in a group specifies that the work group will be outputting `meshletVerticesCount` vertices and `meshletTriangleCount` indices. Then each thread works to populate the mini-buffers.

With 32 threads per group but up to 64 vertices per meshlet, threads use a strided pattern where each thread processes multiple vertices:

| Thread | Iteration 0 | Iteration 1 |
|--------|-------------|-------------|
| 0      | v0          | v32         |
| 1      | v1          | v33         |
| 2      | v2          | v34         |
| ...    | ...         | ...         |
| 31     | v31         | v63         |


The vertex loop first fetches the vertex indirection index from the meshlet vertex buffer, and uses that to index into the real vertex buffer. The resulting vertex is then processed into the output mini-vertex buffer like a normal vertex shader would.

The triangle loop will then simply forward the triangle indices from the meshlet buffer, fairly straightforward.

We dispatch one mesh shader group per meshlet, which is 2,251 groups total for the Stanford Bunny:

````````````````````````
MeshOnlyPipelinePushConstant pushData{
    .modelMatrix = stanfordBunny.transform.GetMatrix(),
    .sceneData = currentSceneDataBuffer.address,
    .vertexBuffer = vertexBuffer.address,
    .meshletVerticesBuffer = meshletVerticesBuffer.address,
    .meshletTrianglesBuffer = meshletTrianglesBuffer.address,
    .meshletBuffer = meshletBuffer.address,
};

vkCmdPushConstants(cmd, meshOnlyPipeline.pipelineLayout.handle, VK_SHADER_STAGE_MESH_BIT_EXT, 0, sizeof(MeshOnlyPipelinePushConstant), &pushData);
vkCmdDrawMeshTasksEXT(cmd, stanfordBunny.meshlets.size(), 1, 1);
````````````````````````

![Mesh shader pipeline: CPU dispatch spawns groups, each outputs vertex/index data for rasterization](MeshOnly.png)

This works well, but still renders every meshlet regardless of visibility. This is where task shaders come in.

# Adding Task Shaders and Culling



# Conclusion


One common source of confusion is what a task/mesh shader should receive as input and what they should produce as output.

**Task Shader**

- Receives N meshlet indices as input (configured by programmer via group size)
- Evaluates visibility for those N meshlets (frustum culling, backface culling)
- Dispatches [0, N] mesh shader groups - one per visible meshlet
- This dispatch should either be executed by a single thread or all threads should dispatch the same parameters. It is undefined behavior otherwise.


**Mesh Shader**

- Each mesh shader group takes 1 meshlet. Which meshlet it works on is for the programmer to work out, typically decided by the task shader and communicated through the payload.
-

<!-- Markdeep: --><style class="fallback">body {visibility: hidden;white-space: pre;font-family: monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep || (document.body.style.visibility = "visible")</script>