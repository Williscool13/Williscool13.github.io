<meta charset="utf-8" emacsmode="-*- markdown -*-"> <link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/journal.css?">

						**[Will's Journal](https://www.williscool.com)**
						
						
						
						
						
						
**2024/06/04: MSAA, Transparency/Cutoff, Shader Object, and Draw Image Resize**
=================================================================

I've pretty much finished VkGuide (pending optimization techniques) and have gone forth into the engine development world.

(###) **Shader Objects**

The first thing I implemented was shader objects. It was surprisingly easy to swap out traditional pipelines to this new shader object. Shader Objects is likely the last
shiny new feature that I'll be implementing for my game engine. From here on out, it will be developing features and implementing cool new techniques from research papers!
Shader Objects are supposed to be faster or equal to traditional pipelines when it comes to performance, but comes with additional flexibility when it comes to setting up pipelines.
Changing a shader won't require a full pipeline rebuild now. 

Fortunately, shader objects are 100% compatible with descriptor buffers, buffer device addresses, and dynamic rendering. It would've been devastating to have to throw away some complex features.

(###) **MSAA**

Finally implemented MSAA! It was easy enough to set up. MSAA is has dedicated hardware and is well supported by most rendering APIs. Of course, my development was once again slowed by documentation.
There seems to have been some API changes, as the "samples" property is no longer in the render attachment info. Rather it is found during instantiation of the image itself. 
Additionally, using shader objects meant to change the state of the pipeline, I also had to call **vkCmdSetRasterizationSamplesEXT(cmd, _rasterizationSamples);**. This was fairly easy to find; the new Vulkan Docs are wonderful.

After initially setting up MSAA, my graphics pipeline was directly over-writing the background drawn by my compute pipeline. The compute shader draws to the image through a storage image descriptor.
In order for MSAA to work, the render attachment image to the pipeline must have VK_SAMPLE_COUNT_X_BIT (where X is sample count). 
However, storage images are required to be VK_SAMPLE_COUNT_1_BIT. So how do I convert a VK_SAMPLE_COUNT_1_BIT image to a VK_SAMPLE_COUNT_X_BIT image?

My solution: A basic fullscreen shader! Though its implementation wasn't exactly flawless. It exposed some issues with window resolution modifications and came with a performance cost. Some basic profiling determined that it amounted to about
0.25ms. That seems expensive! It seems that using compute shaders to draw backgrounds has some cost associated with it!

(###) **Transparency/Cutoff**

VkGuide's project already came with a fairly complete implementation of blending, though its focus was primarily additive blending. I managed to get alpha blending to work fairly easily.
While I was at it I decided to also implement alpha cutoff, which is basically opaque with a threshold cutoff for fragments with non 1 alpha. I passed the cutoff value to the fragment shader through push constants.

Push constants are great but I think I'm too liberal with its use. The Vulkan spec guarantees that push constants have 128 bytes of memory. 
I use 64 bytes for the model matrix, 48 bytes for the normal-model matrix (mat3x4 for alignment, couldn't seem to get alignas to work), and 8 bytes for buffer device address.
With 4 bytes for the new cutoff threshold I'm up to 124 bytes, with enough space for only 1 more float. I wonder what I'll be using that for.


![gltf Sample AlphaBlendModeTest](images/transparencyAndCutoff.png)


(###) **Draw Image Resize**

This one might not make sense outside of the context of my engine. VkGuide has you set a separate draw image as the render target of your pipelines, and as a final step in the draw call, blit that draw image onto the swapchain image.
I think this is a nice idea even though it may not be immediately useful. 

In the past, when rescaling your window, the draw image stayed at a constant 1700x900 resolution, only the swapchain image would change. If the resolution was say, 500x500, you would only draw to the first 500x500 of the draw image, resulting in an unused
portion of the draw image. This is likely more efficient than constantly changing the draw image with each resolution change, but it happens fairly infrequently, so I decided to change it anyway.





**2024/05/30: Remarks About Vulkan**
=================================================================

It has been over week since I started VkGuide, and I am wired in on learning Vulkan. It is interesting to see how a modern
rendering API works in comparison to old school OpenGL 3.3. Below are some remarks about Vulkan based on my limited understanding of rendering APIs.

(###) **Data buffers**

VkGuide uses a different technique to pass vertex data to the pipeline. Opting out of using the typical vkCmdDraw or vkCmdBindVertexBuffers and instead using 
descriptor indexing. The guide passes the buffer location (GPU pointer) of the vertex buffer to the pipeline through push constants. The way this is done is gnarly. 
I don't have a great amount of experience working directly in memory, so it took me quite a bit to reasonably grasp what was happening. Yet, I feel refreshed when facing 
memory-facing code, because it all makes sense - all the way down to hardware level. In fact, it intrigued me so much that I decided to opt out of using Descriptor Pools and 
once again go straight to memory with Descriptor Buffers; more on that later.

I really enjoy the idea of buffer references, it maps very intuitively to GPU memory and simplifies the process of allocating and accessing data. 
This of course comes with the caveat that you need to be careful not to go out of bounds. Though with vertex indices this shouldn't be an issue (assuming the indices were automatically generated). 

(###) **API Changes**

This one isn't about Vulkan as it is about the ecosystem surrounding Vulkan. Why must there always be so much volatility in the API of libaries? My gripe this time is about fastgltf. 
The library is a terrific tool to efficiently and rapidly load gltf models; a task which it excelled at in my project. However, why isn't there a clear way to use it?
Their documentation is out of date and VkGuide's instructions contain functions and commands that seem to be from an older version of the API. To find the correct code I had to look at fastgltf's samples.
It can be frustrating trying to get things done when documentation is so sparsely updated.

Vulkan also seems to be continuously updating - not that this is bad. I think it's great that Vulkan always moves to improve the interface between application and GPU, all in the name of speed and control.
From renderpass/framebuffer (which I haven't learned) to dynamic rendering, pipelines to shader objects, descriptor sets to descriptor buffers, buffer attachments to buffer references.
This makes developing in Vulkan feel incomplete; I want to use the newest and fastest tools, I like shiny new things. But at some point I will need to plant my feet and start developing with what I have.
Development can't proceed to a reasonable level if I keep adapting my codebase to support every shiny new tool.

(###) **Using Compute Shader to Draw**

The Compute shader pipeline used to draw the background of the application produced mixed results.
When I change my drawExtent to 500x500, the drawImage doesn't change (it's a little annoying to remake the drawImage whenever the screen rescales) and remains at the default 1700x900. 
The shader code might help shed some light on why the background may not look how you expect it to.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#version 460
layout (local_size_x = 16, local_size_y = 16) in;
layout(rgba16f,set = 0, binding = 0) uniform image2D image;

//push constants block
layout( push_constant ) uniform constants
{
 vec4 data1;
 vec4 data2;
 vec4 data3;
 vec4 data4;
} PushConstants;

void main() 
{
    ivec2 texelCoord = ivec2(gl_GlobalInvocationID.xy);
	ivec2 size = imageSize(image);
    vec4 topColor = PushConstants.data1;
    vec4 bottomColor = PushConstants.data2;
    if(texelCoord.x < size.x && texelCoord.y < size.y)
    {
        float blend = float(texelCoord.y)/(size.y); 
        imageStore(image, texelCoord, mix(topColor,bottomColor, blend));
    }
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The shader blends based on the resolution of the draw image! Not the resolution of the drawExtent! Of course, this would easily be fixed if I passed drawExtent to the compute shader to 
fix the blend factor, but this result didn't bother me. Just a cute little quirk of the compute pipeline implementation in this project.
![Full Resolution](images/computeBackgroundMaxRes.png)![10% Resolution](images/computeBackgroundLowRes.png)
The geometry pipeline was unaffected by this because when rendering you need to specify the drawExtents of the attached images.
VkRenderingInfo renderInfo = vkinit::rendering_info(_drawExtent, &colorAttachment, &depthAttachment);

(###) **Reverse Depth Buffer**

Depth buffer funny-business is no stranger to me. I have seen and read all about it when attempting to create toon shaders and from my time learning OpenGL. Yet, the reverse depth buffer still caused me some grief. 
Of course, I understand why it is done: It helps depth precision due to the non-linear distribution. But setting it up cause more issues than it should have; even now, I'm not sure why it took me so long to do 3 things:

 - If using GLM do **`#define GLM_FORCE_DEPTH_ZERO_TO_ONE`** before you include the matrix header files.
 - Swap near and far in glm::perspective 
  - **`glm::mat4 proj = glm::perspective(glm::radians(70.0f), (float)_windowExtent.width / (float)_windowExtent.height, 10000.0f, 0.1f);`**
 - Change depth stencil reset value to 0 
  - **`VkClearValue depthClearValue = { 0.0f, 0 };`**
 - Change depth comparison operator to **`VK_COMPARE_OP_GREATER_OR_EQUAL`** when building your pipeline 
  - **`pipelineBuilder.enable_depthtest(true, VK_COMPARE_OP_GREATER_OR_EQUAL);`**
 
(###) **Descriptor Buffer**
 
After a few days of tinkering with it and mutilating the VkGuide project code, I have successfully set up descriptor buffers to be fully working. I can't believe I did it! While setting it up, I couldn't help
feel lost because of how little information exists out there about it. Is noone talking about it? I don't see many forum posts about it. It is fairly new, maybe there won't be many more forum posts because the age 
of the forum is dead? Guess we can thank LLMs for that.
 
So I decided to write a blog post about it! You can find it here on my website or on medium. I'm sure you can find the links yourselves. My project comes with a wrapper for descriptor buffers that I think could be useful
if you would like a jumping off point for your own descriptor buffer implementations. Just follow the link from the descriptor buffer blog post.

(###) **Diagram of GPU pipeline**

And I will finish this entry with a small diagram of a typical pipeline in vulkan.

******************************************************************************************************************************
*+-----------------------------------------------+
*|               Vertex Input                    |
*+-----------------------------------------------+
*|              Input Assembly                   |
*+-----------------------------------------------+
*|               Vertex Shader                   |
*+-----------------------------------------------+
*|           (Optional) Tessellation             |
*+-----------------------------------------------+
*|         (Optional) Geometry Shader            |
*+-----------------------------------------------+
*|               Rasterization                   |
*+-----------------------------------------------+
*|                 Clipping                      |  <-- Implicit
*+-----------------------------------------------+
*|           Perspective Divide                  |  <-- Implicit
*+-----------------------------------------------+
*|        Primitive Assembly (part)              |  <-- Implicit
*+-----------------------------------------------+
*|         Viewport Transformation               |  <-- Implicit
*+-----------------------------------------------+
*|             Fragment Generation               |  <-- Implicit
*+-----------------------------------------------+
*|          Early Fragment Tests                 |  <-- Implicit
*+-----------------------------------------------+
*|               Fragment Shader                 |
*+-----------------------------------------------+
*|           Late Fragment Tests                 |  <-- Implicit
*+-----------------------------------------------+
*|               Color Blending                  |
*+-----------------------------------------------+
*|            Pixel Ownership Test               |  <-- Implicit
*+-----------------------------------------------+
*|             Viewport and Scissor              |
*+-----------------------------------------------+
******************************************************************************************************************************






**2024/05/24: Vulkan Engine (VKGuide)**
=================================================================
	(##) **Vulkan API**
	
	I had found myself delaying my goals for no good reason. After wallowing in my despair for an adequate amount of time, I decided I would finally
	begin learning and using Vulkan API as part of my journey to create my own game engine. 
	Vulkan has a reputation for being both harder and more verbose than OpenGL, so learning it would be a marathon more than a sprint. While knowing OpenGL has helped immensely
	in both understanding the structure of a rendering pipeline and coding in c++, it did not equip me for Vulkan.
	
	(###) **Boilerplate**
	
	Vulkan just has so much adjustable values everywhere! Flags, references, libraries; there are a lot of moving parts and it all needs to be constructed before you can even begin
	development.
	VKGuide is useful, but the guide is still sort of rough around the edges, with a few overlooked points and minor typos. It can be hard to learn if you are unsure of whether the things 
	you read are true. Still, it is a fantastic resource to get started with Vulkan. It also seems to be more geared towards game engines, which aligns with my goals well.
	
	Some parts of Vulkan still puzzle me, such as synchronization (barriers). But sentiment online seem to suggest that Vulkan's synchronization is a difficult subject to fully grasp. But difficulty 
	only serves to motivate me. I love the struggle; the fight. 
	
	(###) **Compute Shaders**
	
	While maybe not the best idea, I did not mess with compute shaders before I hopped over to Vulkan. Mostly because it seemed a significantly complex subject. 
	VKGuide starts with compute shaders! I can understand why. From reading the docs, setting up a compute shader pipeline is significantly easier than setting up a rendering pipeline. 
	_Though the hard part was the setup of the rest of the program, really._ Compute shaders are actually remarkably simple, and to me is just a juiced up CPU that better exposes SIMD architecture to the
	developer, simplifying the multithreading aspect of programming. I think it is a fairly neat bit of program, and really, if you boil shaders to their finer bits, they are basically compute shaders
	with pre-defined structures to support the rendering pipeline.
	
	(###) **Docs**
	
	I really should have started my Vulkan journey by taking a quick look at the official documentation. Specifically the first few chapters: Introduction, Fundamentals, and Command Buffers; if I had read them
	before starting VKGuide, I feel like I would have had an easier time grasping concepts. Certain things that VKGuide describe seem arbitrary, and can be light on further details. It can make it hard
	to understand why we do things without a clear image of the bigger picture. The docs shed some light on why certain things are the way that they are.
	
	(###) **Project Structure**
	
	I thought this would be a nice time to demonstrate some MarkDeep magic and below is a graph describing the project's structure so far (Compute shader pipeline).
	
******************************************************************************************************************************
*                        .------------------.                                                                       
*   .----------.         |  Vulkan          |                                                  .--------------------.                 	
*   |  SDL     |         | > Instance       |        .------------------------------.          |  Commands          |
*   | > Init   |-------> | > Surface        |        |  Swapchain                   |--------> | > Command Pool     | 
*   | > Window |         | > PhysDevice     |------> | > Swapchain Object           |          | > Command Buffers  |
*   '----------'         | > Device         |        | > Swapchain Images/Views     |          '--------------------'
*                        | > Graphics Queue |        | > Draw Image (Render Target) |                    |          
*                        | > Queue Family   |        '------------------------------'                    |           
*                        |                  |                                                            v             
*                        | > VMA            |                                                                       
*                        '------------------'                                               .-------------------------------.
*                                                                                           |  Syncronization Structure     |
*                                                                                           | > Fence (Command Buffer)      |
*                                                                                           | > Semaphore (Swapchain Image) |
*                                                     .--------------------------.          | > Semaphore (Render Finish)   |
*                                                     |  Descriptors             |          '-------------------------------'
*                .-------------------.                | > Descriptor Pool        |                        |                   
*                |  Pipeline         | <--------------| > Descriptor Set         | <----------------------'                  
*                | > Shaders         |                |   > Populate Set w/ Data |                                     
*                | > Pipeline Object |                '--------------------------'                                          
*                '-------------------'                                                                                        
*                                                                                                                             
******************************************************************************************************************************
	
	With all the pieces in place at the end of this structure, the draw step can finally be executed.
	Apart from the uninteresting synchronization setup, the juice of the program exists here.
	
	```````````````````````````
	void VulkanEngine::draw_background(VkCommandBuffer cmd)
	{
    ComputeEffect& selected = backgroundEffects[currentBackgroundEffect];
    // Bind Pipeline
    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, selected.pipeline);
    // Push Constants
    vkCmdPushConstants(cmd, _gradientPipelineLayout, VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(ComputePushConstants), &selected._data);
    // Bind Descriptor Set
    vkCmdBindDescriptorSets(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, _gradientPipelineLayout, 0, 1, &_drawImageDescriptorSet, 0, nullptr);
    // Execute at 8x8 thread groups
    vkCmdDispatch(cmd, std::ceil(_drawExtent.width / 8.0), std::ceil(_drawExtent.height / 8.0), 1);
	}
	```````````````````````````
	- At this point in the guide, we use 2 compute shaders to show off how easy it is to change between the pipelines (toggled by a slider from imgui)
	- Bind the chosen pipeline
	- Attach Push Constants
	- Bind Descriptor Set
	- Dispatch to 64 (8x8) (GPU) threads
	
	Vulkan is great. So much control over the entire rendering structure in the hands of the developer, allowing for better optimization and a closer map between software and hardware.
	
	![Hello Triangle (Compute Shader)](images/vulkan/computeTriangle.png) ![Random Shader from ShaderToy](images/vulkan/computeStarry.png)
	
	
**2024/04/30: Past activities**
=================================================================

	(##) **Unity Shader Tutorials**
	
	I began by following tutorials about rendering techniques, 
	particularly from [NedMakesGames](https://www.youtube.com/@NedMakesGames) 
	and [Ben Cloward](https://www.youtube.com/channel/UCoG9TB1eL6dm9eNbLFueHBQ).
	I had an interest in toon shaders, as it provided an alternative to 
	photo-realistic rendering. Producing images at both a lower cost and
	at reduced complexity. 
	As it turns out, toon shading can be reasonably complex to implement,
	especially since Unity doesn't come with one of the box, requiring users to write
	their own implementations with the rendering pipeline.
	As a beginner with little knowledge on the rendering pipeline, this was a 
	daunting task, which resulted in me doing little more than copying what
	the tutorial had outlined. 
	
	But copying, simply wasn't good enough for me. 
	While both Ned and Ben's tutorials were instrumental in producing the toon shading I wanted, my knowledge was lacking and I knew it.
	I don't enjoy just doing, I enjoy knowing. And to know, will require much more than brief condensed tutorials on youtube. 
	So I decided to embark on a journey to deepen my knowledge.
	
	
	(##) **Real-Time Rendering, 4th Edition**
	
	This led me to the highly recommended book: [Real-Time Rendering](https://www.realtimerendering.com/index.html).
	It was long, and it took me many months to read it front to back. Perhaps it wasn't the best idea to jump right into it
	without studying graphics programming basics a little more. Nonetheless, it was a solid read and I had grasped the information
	that I needed to have a general understanding of graphics programming. 
	
	Truthfully, some of the chapters were a slog to get through. I read them anyway and have come out with a better understanding
	of what inherently piques my interest. Some noteworthy chapters that I find less interesting include: 
	- Volume Rendering 
	- Curves
	- Graphics Hardware
	
	I think that's good news! It means that I find just about everything else that's important interesting.
	
	Some chapters that I find particularly interesting include:
	- Shadows
	- Physically Based Shading
	- Non-Photorealistic Rendering
	
	
	The only problem with reading this lengthy book is that I had come out of it with knowledge, but no produced works.
	What good is knowing what you know, if you don't use it for some purpose? 
	
	(##) **CS6610 - Interactive Computer Graphics by Cem Yuksel**
	
	A name that frequently comes up in a lot of places, Cem Yuksel has a notable presence in the Computer Graphics scene. 
	[His course](https://graphics.cs.utah.edu/courses/cs6610/spring2021/) was recommended on the Real-Time Rendering resource page and gives an introduction to rendering in realtime using OpenGL.
	
	I did this module alongside reading the book and had completed every single one of the projects outlined on the course website.
	The techniques that were required to be implemented we not significantly complex, but as a beginner it was somewhat difficult to get used to.
	
	I wouldn't assume anyone would want to look at code written to complete the minimum requirements of the project, but it is [publicly available](https://github.com/Williscool13/LearnOpenGL).
	
	Techniques implemented include:
	- Texturing
	- Reflections (Not Real)
	- Environment Mapping
	- Shadow Mapping
	- Displacement Mapping
	- Tessellation 
	
	And here are some pictures:
	![Reflections and Environment Mapping](images/reflections.png)![Directional Light](images/directionalLight.png)![Point Light on Normal Mapped Plane](images/pointHeight.png)![Tessellated Vertex Displacement](images/tessellatedVertexDisplacement.png)
	
	(##) **Ray Tracing In a Weekend and The Next Week**
	
	Briefly after finishing Real-Time Rendering, I had decided to look into alternative rendering techniques, specifically ray tracing.
	Ray tracing is a big field of study right now, both in real-time and offline. And while it is slightly more suited
	to offline rendering at the moment, its significantly impressive results make it very tempting to incorporate into 
	real-time rendering. 
	
	Both courses can be found below:
		
	[_Ray Tracing in One Weekend_](https://raytracing.github.io/books/RayTracingInOneWeekend.html)
	
	[_Ray Tracing: The Next Week_](https://raytracing.github.io/books/RayTracingTheNextWeek.html)
	
	This is my most recent activity as of writing. I'm not a particularly artistic person, nor do I enjoy constructing scenes
	to look interesting. So I followed the ray tracing courses closely, and did not significantly modify the scenes.
	Here are a few piectures of the renders:
	
	![Final Render of "In a Weekend"](images/RT1WE_final.png)![Perlin Noise](images/perlin.png)![Simple Lights](images/simpleLight.png)
	
	![Cornell Box](images/cornellBox.png)
	
	My code does look slightly different from the version available on their github. 
	For one, i use stb_image to write the resulting image to a png file. 
	Another change I made is to use templates for vectors and other classes. But I quickly stopped using them,
	as I realized I would never use this code again and I'm writing code that will simply never be used.
	
	The final render of Ray Tracing: The Next Week is being rendered at the moment, so that'll have to wait.
	
	(##) **What's Next?**
	
	As I stand now, I am at a crossroads, with the potential to do anything I want. I know that I would like to create my own game engine. 
	But I don't feel equipped to make it yet. But truthfully, how often do you feel prepared when embarking on a difficult task?
	This choice paralysis is stopping me dead in my tracks, as I am unsure of myself and the steps I need to take to continue my Graphics Programming journey.
	I will end this journal entry with a set of points that I wish to complete over the coming months:
	
	- Learn Vulkan
	- Read [Game Engine Architecture](https://www.gameenginebook.com/)
	- Write my own Game Engine
	- Do Ray Tracing in Real-Time
	
	As for my professional plans in the near future, I will soon be moving to Canada to take a 
	[Graduate Certificate in Game Programming](https://www.sheridancollege.ca/programs/game-development-advanced-programming).
	I'm taking this brief time to study what I personally find interesting to prepare for my future after the course.
	
	[Edit 2024/05/01] 
	Less than 24 hours after writing this post, I received an email saying that the Graduate Certificate program has been suspended for my intake.
	To say that this news is devastating is an understatement. We will have to see how this situation plays out, but this may derail my long-term plans.
 
<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>