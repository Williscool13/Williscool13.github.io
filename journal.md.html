<meta charset="utf-8" emacsmode="-*- markdown -*-"> <link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/journal.css?">

						**[Will's Journal](https://www.williscool.com)**
						
						
						
						
**2024/06/21: Environment Mapping, PBR, and Refactoring**
=================================================================

It has been a while since my last journal entry. I'd like to give an update on what I've been doing over the past 2 weeks and where I think I'll be headed in the next few. After implementing GPU-Driven Rendering, I had decided to look further into
the PBR implementation. Primarily because in a "primitive" scene, the metallic smooth ball was entirely black. I thought it was some sort of mistake, but it is actually because I had no environment. With the previous implementation of PBR,
only the irradiance coming from the scene's lights were included in the calculation of a surface's color (with some additional constant ambient color).

(###) **Environment Maps**

An easy way to approximate the integral to the BRDF terms is to supply the surface calculation with an environment map. More specifically, precomputed cubemaps generated from the surrounding environment. I have in the past implemented environment maps
in OpenGL, so this wasn't too hard a task. All that was left to do is learn how Vulkan manages cubemaps, and additionally how cubemaps interact with the new features I use such as descriptor buffers.

Since these precomputed maps are typically done as a pre-process step, I thought it would be a good idea to do it in another application. I could then package the resulting cubemaps with the final application. So I set out to create a new vulkan application
to convert an equirectangular panoramic image into a cubemap. This would be done in a compute shader, and the application would need to have an interface to easily preview the environment and the resulting precomputations in a scene.

I am still unsure if storing the images as a separate set of textures is the wisest thing to do, maybe it's better to compute and store them in main memory instead?

When creating the new application, I didn't think I'd need every single file from my original application, so I just started a new project from scratch. I ended up copying (basically) every single class/file into the new project. Could've saved myself some time
by just copying in the first place. 

![Equirectangular Image](images/vulkan/panorama.png)![Environment Map](images/vulkan/environmentMap.png)![Environment Map with Spheres](images/vulkan/environmentBalls.png)

The environment map looks slightly less interesting when not in motion. The scene continuously turns, so the user can have a good view of all faces (except top and bottom) of the cubemap. It turned out fairly standard, and I even used a single 
full-screen triangle to generate it, so it's efficient too!

The third image with the metalic/smooth balls isn't yet implemented. This is just what it looks like before the diffuse/specular precomputed maps are incorporated into the PBR equation. As you can see, the metallic smooth balls are entirely flat,
and almost look unshaded. Frankly, without some sort of global illumination, PBR surfaces are simply not it.


(###) **Precomputing Diffuse Irradiance, Specular Pre-Filtered, and the BRDF LUT**

The process of precomputing the cubemaps (well, one of them is a 2d image) involves 3 parts. I did not solve this alone, I had to reference [Learn OpenGL's chapter on PBR](https://learnopengl.com/PBR/Theory) frequently throughout development of this
application. Frankly, my grasp on the underlying mathematics is not stellar. I could tell you why we do the calculations we do, and what each of them contribute to the final surface shade; but I could not tell you how to derive these things from scratch.

(####) **Part 1: Diffuse Irradiance**

Diffuse irradiance is fairly simple. It is basically a cosine weighted convolution across the entire cubemap for each point on the surface. You take a fixed number of samples to determine the color of each point in the final image.
This is a ridiculously expensive task, and can take several seconds if you choose a high (e.g. 1024) resolution for your diffuse map. Of course, time isn't a concern when working on pre-processing tasks, but slow is slow.

Fortunately, the diffuse irradiance map is not significantly noisy or high frequency. You can get appreciable results at a very low resolution - 32x or 64x is perfectly fine. Which is why it can be beneficial to store the diffuse irradiance map 
at one of the lower mip levels of the prefiltered specular mip chain. More on that later.

Picture below is the same scene from above's diffuse irradiance map. The cubemap is on mip level 5 of the prefiltered specular map, which starts at 1024 (which means this diffuse map is 32x32). Not very helpful, but when this image is applied as a crude form
of global illumination on the pbr surfaces, it helps the objects blend in better. Images will come soon.

![Diffuse Irradiance Map](images/vulkan/diffuseMap.png)

(####) **Part 2: Specular Prefiltered**

Specular prefiltered maps involve creating X number of images, where each image has a specified roughness level which affects the sampling technique used to generate the image. Because the image becomes far less detailed at higher roughness levels,
it is farily standard to only create 1 image, and store roughness 0 at the lowest mip level (which is the highest resolution). You would then populate the following mip level with a higher roughness at a lower resolution, etc. Until at mip level 10 (or 
however high you mip chain goes), the 1x1 image is a prefiltered specular with roughness 1.

This one took me longer to do, as I had to reconcile with the fact that in order to use a compute shader to draw on every mip level, I'd need to make an image view for every mip level. It might seem silly, "how else would you do it?", but I figured there
must've been a way to do it by specifying the mip level in the imageStore function or something.

For a base resolution of 1024x1024, the mip chain would be 10 levels long, with the 5th level dedicated to the diffuse irradiance map. This little bump in the mip chain forces me to implement trilinear filtering manually, rather than through the sampler.
Below are some examples of the results you can get when using specular pre-filtered maps. I am sure you are eagerly anticipating when the cubemaps can be seen on those smooth balls.

![Specular Prefiltered Map R: 0](images/vulkan/specZero.png)![Specular Prefiltered Map R: ~1.5](images/vulkan/specOneFive.png)![Specular Prefiltered Map R: 4](images/vulkan/specFour.png)

(####) **Part 3: BRDF LUT**

This one is the easiest to make, as its just a precomputed response of the BRDF terms relative to some properties. This is as far as I've developed when working with PBR. Of course, if you use the same BRDF model for all your PBR surface (which I expect most do), 
then this would basically be a constant LUT for all calculations in your entire application. 

I'd also like to briefly show the code involved to manually sample trilinearly.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#version 450

layout(location = 0) in vec3 fragPosition;
layout(location = 0) out vec4 outColor;

layout(set = 1, binding = 0) uniform samplerCube environmentMap;

const uint MAX_MIP_LEVEL = 9;

layout(push_constant) uniform PushConstants {
    float lod;
    float diffuseMipLevel;
	bool isDiffuse;
    float pad2;
} pushConstants;


void main()
{
    // sample environment map
	vec3 direction = normalize(fragPosition);
	//vec3 envColor = texture(environmentMap, direction).rgb;
	vec3 envColor = vec3(0.0);
	if (pushConstants.isDiffuse){
		envColor = textureLod(environmentMap, direction, pushConstants.diffuseMipLevel).rgb;
	} else {
		float low = floor(pushConstants.lod);
		float high = low + 1.0;
		if (low >= pushConstants.diffuseMipLevel){
			low += 1;
		}
		if (high >= pushConstants.diffuseMipLevel){
			high += 1;
		}

		float frac = fract(pushConstants.lod);

		envColor = mix(textureLod(environmentMap, direction, low).rgb, textureLod(environmentMap, direction, high).rgb, frac);
	}
	
    
	// HDR to sRGB
	envColor = envColor / (envColor + vec3(1.0));
	envColor = pow(envColor, vec3(1.0 / 2.2));

    outColor = vec4(envColor, 1.0);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


(###) **Refactoring**

I also spent a lot of time refactoring, moving several classes into their own wrappers/classes to greatly reduce the size of the "Engine" object. It can be difficult to effectively decouple resources from the Engine, as the engine is where everything happens.
Still, better encapsulation was necessary and is always an ongoing task.
						
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class VulkanResourceConstructor {
public:
	VulkanResourceConstructor() = delete;
	VulkanResourceConstructor(MainEngine* creator);

	// Vulkan Buffers
	AllocatedBuffer create_buffer(size_t allocSize, VkBufferUsageFlags usage, VmaMemoryUsage memoryUsage);
	AllocatedBuffer create_staging_buffer(size_t allocSize);
	void copy_buffer(AllocatedBuffer src, AllocatedBuffer dst, VkDeviceSize size);
	VkDeviceAddress get_buffer_address(AllocatedBuffer buffer);
	void destroy_buffer(const AllocatedBuffer& buffer);

	// Vulkan Images
	AllocatedImage create_image(void* data, size_t dataSize, VkExtent3D size, VkFormat format, VkImageUsageFlags usage, bool mipmapped = false);
	AllocatedImage create_image(VkExtent3D size, VkFormat format, VkImageUsageFlags usage, bool mipmapped = false);
	AllocatedImage create_cubemap(VkExtent3D size, VkFormat format, VkImageUsageFlags usage, bool mipmapped = false);
	void destroy_image(const AllocatedImage& img);

private:
	MainEngine* _creator;
	VkDevice _device;
	VmaAllocator _allocator;
};
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The above is an example of a, while still highly coupled with the engine, allows the length of the main class to be shorter. Of course, one gigantic drawback of working on an adjacent application is that refactors that I would enjoy in the main engine
application will need to be ported over. While not difficult, this can be time consuming. Especially because in my infinite wisdom, I decided to rename the Engine class.

(###) **Shader Code**

I think it is highly likely that in the future, I will need to review the code to generate a fullscreen triangle. While not necessarily difficult, placing it here will reduce some headache in the future.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#version 450

vec3 positions[3] = vec3[](
        vec3(-1.0, -1.0, 0.99), // Bottom-left corner
        vec3( 3.0, -1.0, 0.99), // Far right of the screen, which combined with the third vertex forms a full-screen quad
        vec3(-1.0,  3.0, 0.99)  // Far top of the screen, which combined with the other vertices forms a full-screen quad
);

layout(set = 0, binding = 0) uniform GlobalUniform {
	mat4 view;
	mat4 proj;
	mat4 viewproj;
} sceneData;

layout(location = 0) out vec3 fragPosition;

void main() {
    vec3 pos = positions[gl_VertexIndex];
	gl_Position = vec4(pos, 1.0);
	fragPosition = (inverse(sceneData.viewproj) * vec4(pos, 1.0)).xyz;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One important requirement to use this, is the view matrix must be formatted a specific way. If using glm::lookAt, which I do, the **`center`** will always have to be 0, and the **`eye`** will be the direction the camera faces - so, the real eye - the real camera position.
This little requirement burned some time off development. Also, notable, 0.99 will need to be replaced with 0.01 if using a reverse depth buffer (or -0.99 if using a standard that has depth go from -1 to 1).


						
						
**2024/06/07: GPU-Driven Rendering and Frustum Culling**
=================================================================

(###) **GPU-Driven Rendering**

Though only a brief time since the last journal entry, much has changed with my engine. After reading up on how tasks can be offloaded to the GPU for better parallelization, I decided to attempt to design my pipelines
to be GPU-Driven. VkGuide has a chapter on this but it pertained to the older version of the guide, so I decided to make my own implementation. The sample of [GPU Rendering and Multi-Draw Indirect](https://docs.vulkan.org/samples/latest/samples/performance/multi_draw_indirect/README.html) found in 
Vulkan's official documentation was helpful as a jumping off point, but much work needed to be done to achieve this task.

This process began with thinking of how to structure the data. After some mental torture, I had decided on the following structure:

![](images/vulkan/multidraw1.png)

it is slightly messy, but I thought i should have 5 bindings, 3 used by the graphics pipeline and 3 used by the compute pipeline for frustum culling. The first binding would hold the vertex buffer through a buffer device address,
which if you aren't already using it, I highly recommend. Each vertex would hold all data it needed to draw itself: the material index (which would access the array found in the material_data_buffer), the model matrix, etc.
There were a couple of major problems with this that I couldn't accept.

 1. Having the model matrix in the vertices ballooned the storage requirements of the vertex buffer. 
 2. Not seen here is how the data is managed as a whole. In short, each sub-mesh in a model would create a new bunch of vertices in the Vertex data, further increasing storage costs through redundancy.
 
 Having duplicated data is great if you want small changes that may differ for each sub-mesh in the mesh, but in the majority of cases, creating these duplicates only reduces the amount of GPU memory you have to work with.
 Not far into working on this implementation, these ideas were scrapped.
 
 A big part of the change to the renderer is how the data is organized in the application. There is a whole new class that constructs the data to be more data-oriented, with almost no class representations of the 
 objects. Instead, the data is neatly laid out in arrays to be mapped directly into the GPU. This includes: Images (textures), Samplers, and Materials.

![](images/vulkan/multidraw2.png)![](images/vulkan/multidraw3.png)![](images/vulkan/multidraw4.png)

Every time I refactored the data layout, I updated my diagram. This was mostly done to clearly map out my thought process on how the data would flow in the application. I spent a great deal of time considering what kinds of data are associated with 
what level of granularity. Frequently I had removed something only to immediately add it back in. It was a mess and I am ashamed it took me as long as it did to come up with a suitable data layout. 

Eventually I had settled on 3 layers of granularity: 

- Per Mesh
   - Vertices
   - Textures
   - Samplers
- Per Material
   - Base Color
   - Metallic/Roughness Factors
   - Texture and Sampler Indices
- Per Instance 
   - Model Matrix
   - Vertex Offset and Size (There is some data duplication here, but it is minimal)

Each Mesh would combine the vertices of all sub-meshes into one large vertex buffer and keep track of the offsets and size of each mesh. Then when an instance is created, as specified by the node-based structure of the gltf file, it would store
this offset and size. Additionally, each vertex would have the following properties:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct MultiDrawVertex {
	glm::vec3 position;
	glm::vec3 normal;
	glm::vec4 color;
	glm::vec2 uv;
	uint32_t materialIndex; 
};
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This is because a sub-mesh is comprised of a set number of primitives, and each primitive can be made of a different material. Thus, when creating the vertex buffer, I also included the material index to reduce the number of levels I had to maintain.
This only came at the cost of 4 bytes per vertex, which isn't too much, especially since it can be packed into the gaps of the 16 byte alignments.

The final structure is as follows:

![Words!](images/vulkan/multidraw5.png)![Diagram!](images/vulkan/multidrawDiagram.png)

I think it is fairly efficient, clear, and simple solution. Because I store and access vertices through a buffer device address, instead of supplying a vertex offset to the **`VkDrawIndexedIndirectCommand`**, I instead supply it myself through 
the per instance data. This solution is not only faster than before, but is also allows for easier incorporation of the GPU for techniques such as Frustum and Occlusion Culling. As for memory, this solution uses equal or slightly less memory than 
the old renderer. 

My solution leverages the organization of gltf filesa and minimizes data duplication on GPU memory. Taking a step back to look at the bigger picture, my implementation is not much different from VkGuide's solution. Their solution
has shared resources contained in objects on the CPU, whereas mine stores these shared resources in arrays of VkBuffers.

I can safely say that it turned out much better than I could have ever hoped.

(###) **Frustum Culling**

To be honest, the choice to shift to a GPU-Driven renderer actually came about due to my interest in implementing culling. I noticed that there exists a way to do it with the GPU rather than the CPU and thought: GPU goes brrrr. 
Frustum culling was easy enough to implement, though my bounding sphere algorithm is shamelessly ripped off from the vulkan sample, and isn't the most efficient algorithm. The compute pipeline was inifinitely easier to set up and get working than the 
graphics pipeline, and basically worked upon first iteration, which is unbelievable!

Something that helped me develop the structure faster was working backwards. 
Rather than thinking of the data layout, creating the pipeline, then writing the shaders; I instead started with the shaders, then worked on the data layout and finally the pipeline.
The data's purpose is to be finally processed by the shader, so understanding what the data needed to look like; what kind of data the shaders needed to make the necessary calculations, simplified the process greatly.

When looking at a blank scene with 0 meshes in frame (and hopefully their bounding spheres too), my GPU usage drops from a whopping 95% when drawing 9 copies of the "structure" scene to a staggering 0-10%, 
which is the GPU doing the "GPU-Driven" calculations. 

Whats next? I think I'll be doing some work on a simple shading model to make my scenes look slightly better. I'll also want to do some work on shadows. I did some shadow mapping in the past, but it was extremely brute force and did not scale at all.
I'm planning to tackle cascaded shadow maps for a directional light. Though I am considering potentially looking into deferred rendering, which I have never implemented before. I'll think about it over the next day or so.




**2024/06/04: MSAA, Transparency/Cutoff, Shader Object, and Draw Image Resize**
=================================================================

I've pretty much finished VkGuide (pending optimization techniques) and have gone forth into the engine development world.

(###) **Shader Objects**

The first thing I implemented was shader objects. It was surprisingly easy to swap out traditional pipelines to this new shader object. Shader Objects is likely the last
shiny new feature that I'll be implementing for my game engine. From here on out, it will be developing features and implementing cool new techniques from research papers!
Shader Objects are supposed to be faster or equal to traditional pipelines when it comes to performance, but comes with additional flexibility when it comes to setting up pipelines.
Changing a shader won't require a full pipeline rebuild now. 

Fortunately, shader objects are 100% compatible with descriptor buffers, buffer device addresses, and dynamic rendering. It would've been devastating to have to throw away some complex features.

(###) **MSAA**

Finally implemented MSAA! It was easy enough to set up. MSAA is has dedicated hardware and is well supported by most rendering APIs. Of course, my development was once again slowed by documentation.
There seems to have been some API changes, as the "samples" property is no longer in the render attachment info. Rather it is found during instantiation of the image itself. 
Additionally, using shader objects meant to change the state of the pipeline, I also had to call **vkCmdSetRasterizationSamplesEXT(cmd, _rasterizationSamples);**. This was fairly easy to find; the new Vulkan Docs are wonderful.

After initially setting up MSAA, my graphics pipeline was directly over-writing the background drawn by my compute pipeline. The compute shader draws to the image through a storage image descriptor.
In order for MSAA to work, the render attachment image to the pipeline must have VK_SAMPLE_COUNT_X_BIT (where X is sample count). 
However, storage images are required to be VK_SAMPLE_COUNT_1_BIT. So how do I convert a VK_SAMPLE_COUNT_1_BIT image to a VK_SAMPLE_COUNT_X_BIT image?

My solution: A basic fullscreen shader! Though its implementation wasn't exactly flawless. It exposed some issues with window resolution modifications and came with a performance cost. Some basic profiling determined that it amounted to about
0.25ms. That seems expensive! It seems that using compute shaders to draw backgrounds has some cost associated with it!

(###) **Transparency/Cutoff**

VkGuide's project already came with a fairly complete implementation of blending, though its focus was primarily additive blending. I managed to get alpha blending to work fairly easily.
While I was at it I decided to also implement alpha cutoff, which is basically opaque with a threshold cutoff for fragments with non 1 alpha. I passed the cutoff value to the fragment shader through push constants.

Push constants are great but I think I'm too liberal with its use. The Vulkan spec guarantees that push constants have 128 bytes of memory. 
I use 64 bytes for the model matrix, 48 bytes for the normal-model matrix (mat3x4 for alignment, couldn't seem to get alignas to work), and 8 bytes for buffer device address.
With 4 bytes for the new cutoff threshold I'm up to 124 bytes, with enough space for only 1 more float. I wonder what I'll be using that for.


![gltf Sample AlphaBlendModeTest](images/transparencyAndCutoff.png)


(###) **Draw Image Resize**

This one might not make sense outside of the context of my engine. VkGuide has you set a separate draw image as the render target of your pipelines, and as a final step in the draw call, blit that draw image onto the swapchain image.
I think this is a nice idea even though it may not be immediately useful. 

In the past, when rescaling your window, the draw image stayed at a constant 1700x900 resolution, only the swapchain image would change. If the resolution was say, 500x500, you would only draw to the first 500x500 of the draw image, resulting in an unused
portion of the draw image. This is likely more efficient than constantly changing the draw image with each resolution change, but it happens fairly infrequently, so I decided to change it anyway.





**2024/05/30: Remarks About Vulkan**
=================================================================

It has been over week since I started VkGuide, and I am wired in on learning Vulkan. It is interesting to see how a modern
rendering API works in comparison to old school OpenGL 3.3. Below are some remarks about Vulkan based on my limited understanding of rendering APIs.

(###) **Data buffers**

VkGuide uses a different technique to pass vertex data to the pipeline. Opting out of using the typical vkCmdDraw or vkCmdBindVertexBuffers and instead using 
descriptor indexing. The guide passes the buffer location (GPU pointer) of the vertex buffer to the pipeline through push constants. The way this is done is gnarly. 
I don't have a great amount of experience working directly in memory, so it took me quite a bit to reasonably grasp what was happening. Yet, I feel refreshed when facing 
memory-facing code, because it all makes sense - all the way down to hardware level. In fact, it intrigued me so much that I decided to opt out of using Descriptor Pools and 
once again go straight to memory with Descriptor Buffers; more on that later.

I really enjoy the idea of buffer references, it maps very intuitively to GPU memory and simplifies the process of allocating and accessing data. 
This of course comes with the caveat that you need to be careful not to go out of bounds. Though with vertex indices this shouldn't be an issue (assuming the indices were automatically generated). 

(###) **API Changes**

This one isn't about Vulkan as it is about the ecosystem surrounding Vulkan. Why must there always be so much volatility in the API of libaries? My gripe this time is about fastgltf. 
The library is a terrific tool to efficiently and rapidly load gltf models; a task which it excelled at in my project. However, why isn't there a clear way to use it?
Their documentation is out of date and VkGuide's instructions contain functions and commands that seem to be from an older version of the API. To find the correct code I had to look at fastgltf's samples.
It can be frustrating trying to get things done when documentation is so sparsely updated.

Vulkan also seems to be continuously updating - not that this is bad. I think it's great that Vulkan always moves to improve the interface between application and GPU, all in the name of speed and control.
From renderpass/framebuffer (which I haven't learned) to dynamic rendering, pipelines to shader objects, descriptor sets to descriptor buffers, buffer attachments to buffer references.
This makes developing in Vulkan feel incomplete; I want to use the newest and fastest tools, I like shiny new things. But at some point I will need to plant my feet and start developing with what I have.
Development can't proceed to a reasonable level if I keep adapting my codebase to support every shiny new tool.

(###) **Using Compute Shader to Draw**

The Compute shader pipeline used to draw the background of the application produced mixed results.
When I change my drawExtent to 500x500, the drawImage doesn't change (it's a little annoying to remake the drawImage whenever the screen rescales) and remains at the default 1700x900. 
The shader code might help shed some light on why the background may not look how you expect it to.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#version 460
layout (local_size_x = 16, local_size_y = 16) in;
layout(rgba16f,set = 0, binding = 0) uniform image2D image;

//push constants block
layout( push_constant ) uniform constants
{
 vec4 data1;
 vec4 data2;
 vec4 data3;
 vec4 data4;
} PushConstants;

void main() 
{
    ivec2 texelCoord = ivec2(gl_GlobalInvocationID.xy);
	ivec2 size = imageSize(image);
    vec4 topColor = PushConstants.data1;
    vec4 bottomColor = PushConstants.data2;
    if(texelCoord.x < size.x && texelCoord.y < size.y)
    {
        float blend = float(texelCoord.y)/(size.y); 
        imageStore(image, texelCoord, mix(topColor,bottomColor, blend));
    }
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The shader blends based on the resolution of the draw image! Not the resolution of the drawExtent! Of course, this would easily be fixed if I passed drawExtent to the compute shader to 
fix the blend factor, but this result didn't bother me. Just a cute little quirk of the compute pipeline implementation in this project.
![Full Resolution](images/computeBackgroundMaxRes.png)![10% Resolution](images/computeBackgroundLowRes.png)
The geometry pipeline was unaffected by this because when rendering you need to specify the drawExtents of the attached images.
VkRenderingInfo renderInfo = vkinit::rendering_info(_drawExtent, &colorAttachment, &depthAttachment);

(###) **Reverse Depth Buffer**

Depth buffer funny-business is no stranger to me. I have seen and read all about it when attempting to create toon shaders and from my time learning OpenGL. Yet, the reverse depth buffer still caused me some grief. 
Of course, I understand why it is done: It helps depth precision due to the non-linear distribution. But setting it up cause more issues than it should have; even now, I'm not sure why it took me so long to do 3 things:

 - If using GLM do **`#define GLM_FORCE_DEPTH_ZERO_TO_ONE`** before you include the matrix header files.
 - Swap near and far in glm::perspective 
  - **`glm::mat4 proj = glm::perspective(glm::radians(70.0f), (float)_windowExtent.width / (float)_windowExtent.height, 10000.0f, 0.1f);`**
 - Change depth stencil reset value to 0 
  - **`VkClearValue depthClearValue = { 0.0f, 0 };`**
 - Change depth comparison operator to **`VK_COMPARE_OP_GREATER_OR_EQUAL`** when building your pipeline 
  - **`pipelineBuilder.enable_depthtest(true, VK_COMPARE_OP_GREATER_OR_EQUAL);`**
 
(###) **Descriptor Buffer**
 
After a few days of tinkering with it and mutilating the VkGuide project code, I have successfully set up descriptor buffers to be fully working. I can't believe I did it! While setting it up, I couldn't help
feel lost because of how little information exists out there about it. Is noone talking about it? I don't see many forum posts about it. It is fairly new, maybe there won't be many more forum posts because the age 
of the forum is dead? Guess we can thank LLMs for that.
 
So I decided to write a blog post about it! You can find it here on my website or on medium. I'm sure you can find the links yourselves. My project comes with a wrapper for descriptor buffers that I think could be useful
if you would like a jumping off point for your own descriptor buffer implementations. Just follow the link from the descriptor buffer blog post.

(###) **Diagram of the GPU pipeline**

And I will finish this entry with a small diagram of a typical pipeline in vulkan.

******************************************************************************************************************************
*+-----------------------------------------------+
*|               Vertex Input                    |
*+-----------------------------------------------+
*|              Input Assembly                   |
*+-----------------------------------------------+
*|               Vertex Shader                   |
*+-----------------------------------------------+
*|           (Optional) Tessellation             |
*+-----------------------------------------------+
*|         (Optional) Geometry Shader            |
*+-----------------------------------------------+
*|               Rasterization                   |
*+-----------------------------------------------+
*|                 Clipping                      |  <-- Implicit
*+-----------------------------------------------+
*|           Perspective Divide                  |  <-- Implicit
*+-----------------------------------------------+
*|        Primitive Assembly (part)              |  <-- Implicit
*+-----------------------------------------------+
*|         Viewport Transformation               |  <-- Implicit
*+-----------------------------------------------+
*|             Fragment Generation               |  <-- Implicit
*+-----------------------------------------------+
*|          Early Fragment Tests                 |  <-- Implicit
*+-----------------------------------------------+
*|               Fragment Shader                 |
*+-----------------------------------------------+
*|           Late Fragment Tests                 |  <-- Implicit
*+-----------------------------------------------+
*|               Color Blending                  |
*+-----------------------------------------------+
*|            Pixel Ownership Test               |  <-- Implicit
*+-----------------------------------------------+
*|             Viewport and Scissor              |
*+-----------------------------------------------+
******************************************************************************************************************************






**2024/05/24: Vulkan Engine (VKGuide)**
=================================================================
	(##) **Vulkan API**
	
	I had found myself delaying my goals for no good reason. After wallowing in my despair for an adequate amount of time, I decided I would finally
	begin learning and using Vulkan API as part of my journey to create my own game engine. 
	Vulkan has a reputation for being both harder and more verbose than OpenGL, so learning it would be a marathon more than a sprint. While knowing OpenGL has helped immensely
	in both understanding the structure of a rendering pipeline and coding in c++, it did not equip me for Vulkan.
	
	(###) **Boilerplate**
	
	Vulkan just has so much adjustable values everywhere! Flags, references, libraries; there are a lot of moving parts and it all needs to be constructed before you can even begin
	development.
	VKGuide is useful, but the guide is still sort of rough around the edges, with a few overlooked points and minor typos. It can be hard to learn if you are unsure of whether the things 
	you read are true. Still, it is a fantastic resource to get started with Vulkan. It also seems to be more geared towards game engines, which aligns with my goals well.
	
	Some parts of Vulkan still puzzle me, such as synchronization (barriers). But sentiment online seem to suggest that Vulkan's synchronization is a difficult subject to fully grasp. But difficulty 
	only serves to motivate me. I love the struggle; the fight. 
	
	(###) **Compute Shaders**
	
	While maybe not the best idea, I did not mess with compute shaders before I hopped over to Vulkan. Mostly because it seemed a significantly complex subject. 
	VKGuide starts with compute shaders! I can understand why. From reading the docs, setting up a compute shader pipeline is significantly easier than setting up a rendering pipeline. 
	_Though the hard part was the setup of the rest of the program, really._ Compute shaders are actually remarkably simple, and to me is just a juiced up CPU that better exposes SIMD architecture to the
	developer, simplifying the multithreading aspect of programming. I think it is a fairly neat bit of program, and really, if you boil shaders to their finer bits, they are basically compute shaders
	with pre-defined structures to support the rendering pipeline.
	
	(###) **Docs**
	
	I really should have started my Vulkan journey by taking a quick look at the official documentation. Specifically the first few chapters: Introduction, Fundamentals, and Command Buffers; if I had read them
	before starting VKGuide, I feel like I would have had an easier time grasping concepts. Certain things that VKGuide describe seem arbitrary, and can be light on further details. It can make it hard
	to understand why we do things without a clear image of the bigger picture. The docs shed some light on why certain things are the way that they are.
	
	(###) **Project Structure**
	
	I thought this would be a nice time to demonstrate some MarkDeep magic and below is a graph describing the project's structure so far (Compute shader pipeline).
	
******************************************************************************************************************************
*                        .------------------.                                                                       
*   .----------.         |  Vulkan          |                                                  .--------------------.                 	
*   |  SDL     |         | > Instance       |        .------------------------------.          |  Commands          |
*   | > Init   |-------> | > Surface        |        |  Swapchain                   |--------> | > Command Pool     | 
*   | > Window |         | > PhysDevice     |------> | > Swapchain Object           |          | > Command Buffers  |
*   '----------'         | > Device         |        | > Swapchain Images/Views     |          '--------------------'
*                        | > Graphics Queue |        | > Draw Image (Render Target) |                    |          
*                        | > Queue Family   |        '------------------------------'                    |           
*                        |                  |                                                            v             
*                        | > VMA            |                                                                       
*                        '------------------'                                               .-------------------------------.
*                                                                                           |  Syncronization Structure     |
*                                                                                           | > Fence (Command Buffer)      |
*                                                                                           | > Semaphore (Swapchain Image) |
*                                                     .--------------------------.          | > Semaphore (Render Finish)   |
*                                                     |  Descriptors             |          '-------------------------------'
*                .-------------------.                | > Descriptor Pool        |                        |                   
*                |  Pipeline         | <--------------| > Descriptor Set         | <----------------------'                  
*                | > Shaders         |                |   > Populate Set w/ Data |                                     
*                | > Pipeline Object |                '--------------------------'                                          
*                '-------------------'                                                                                        
*                                                                                                                             
******************************************************************************************************************************
	
	With all the pieces in place at the end of this structure, the draw step can finally be executed.
	Apart from the uninteresting synchronization setup, the juice of the program exists here.
	
	```````````````````````````
	void VulkanEngine::draw_background(VkCommandBuffer cmd)
	{
    ComputeEffect& selected = backgroundEffects[currentBackgroundEffect];
    // Bind Pipeline
    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, selected.pipeline);
    // Push Constants
    vkCmdPushConstants(cmd, _gradientPipelineLayout, VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(ComputePushConstants), &selected._data);
    // Bind Descriptor Set
    vkCmdBindDescriptorSets(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, _gradientPipelineLayout, 0, 1, &_drawImageDescriptorSet, 0, nullptr);
    // Execute at 8x8 thread groups
    vkCmdDispatch(cmd, std::ceil(_drawExtent.width / 8.0), std::ceil(_drawExtent.height / 8.0), 1);
	}
	```````````````````````````
	- At this point in the guide, we use 2 compute shaders to show off how easy it is to change between the pipelines (toggled by a slider from imgui)
	- Bind the chosen pipeline
	- Attach Push Constants
	- Bind Descriptor Set
	- Dispatch to 64 (8x8) (GPU) threads
	
	Vulkan is great. So much control over the entire rendering structure in the hands of the developer, allowing for better optimization and a closer map between software and hardware.
	
	![Hello Triangle (Compute Shader)](images/vulkan/computeTriangle.png) ![Random Shader from ShaderToy](images/vulkan/computeStarry.png)
	
	
**2024/04/30: Past activities**
=================================================================

	(##) **Unity Shader Tutorials**
	
	I began by following tutorials about rendering techniques, 
	particularly from [NedMakesGames](https://www.youtube.com/@NedMakesGames) 
	and [Ben Cloward](https://www.youtube.com/channel/UCoG9TB1eL6dm9eNbLFueHBQ).
	I had an interest in toon shaders, as it provided an alternative to 
	photo-realistic rendering. Producing images at both a lower cost and
	at reduced complexity. 
	As it turns out, toon shading can be reasonably complex to implement,
	especially since Unity doesn't come with one of the box, requiring users to write
	their own implementations with the rendering pipeline.
	As a beginner with little knowledge on the rendering pipeline, this was a 
	daunting task, which resulted in me doing little more than copying what
	the tutorial had outlined. 
	
	But copying, simply wasn't good enough for me. 
	While both Ned and Ben's tutorials were instrumental in producing the toon shading I wanted, my knowledge was lacking and I knew it.
	I don't enjoy just doing, I enjoy knowing. And to know, will require much more than brief condensed tutorials on youtube. 
	So I decided to embark on a journey to deepen my knowledge.
	
	
	(##) **Real-Time Rendering, 4th Edition**
	
	This led me to the highly recommended book: [Real-Time Rendering](https://www.realtimerendering.com/index.html).
	It was long, and it took me many months to read it front to back. Perhaps it wasn't the best idea to jump right into it
	without studying graphics programming basics a little more. Nonetheless, it was a solid read and I had grasped the information
	that I needed to have a general understanding of graphics programming. 
	
	Truthfully, some of the chapters were a slog to get through. I read them anyway and have come out with a better understanding
	of what inherently piques my interest. Some noteworthy chapters that I find less interesting include: 
	- Volume Rendering 
	- Curves
	- Graphics Hardware
	
	I think that's good news! It means that I find just about everything else that's important interesting.
	
	Some chapters that I find particularly interesting include:
	- Shadows
	- Physically Based Shading
	- Non-Photorealistic Rendering
	
	
	The only problem with reading this lengthy book is that I had come out of it with knowledge, but no produced works.
	What good is knowing what you know, if you don't use it for some purpose? 
	
	(##) **CS6610 - Interactive Computer Graphics by Cem Yuksel**
	
	A name that frequently comes up in a lot of places, Cem Yuksel has a notable presence in the Computer Graphics scene. 
	[His course](https://graphics.cs.utah.edu/courses/cs6610/spring2021/) was recommended on the Real-Time Rendering resource page and gives an introduction to rendering in realtime using OpenGL.
	
	I did this module alongside reading the book and had completed every single one of the projects outlined on the course website.
	The techniques that were required to be implemented we not significantly complex, but as a beginner it was somewhat difficult to get used to.
	
	I wouldn't assume anyone would want to look at code written to complete the minimum requirements of the project, but it is [publicly available](https://github.com/Williscool13/LearnOpenGL).
	
	Techniques implemented include:
	- Texturing
	- Reflections (Not Real)
	- Environment Mapping
	- Shadow Mapping
	- Displacement Mapping
	- Tessellation 
	
	And here are some pictures:
	![Reflections and Environment Mapping](images/reflections.png)![Directional Light](images/directionalLight.png)![Point Light on Normal Mapped Plane](images/pointHeight.png)![Tessellated Vertex Displacement](images/tessellatedVertexDisplacement.png)
	
	(##) **Ray Tracing In a Weekend and The Next Week**
	
	Briefly after finishing Real-Time Rendering, I had decided to look into alternative rendering techniques, specifically ray tracing.
	Ray tracing is a big field of study right now, both in real-time and offline. And while it is slightly more suited
	to offline rendering at the moment, its significantly impressive results make it very tempting to incorporate into 
	real-time rendering. 
	
	Both courses can be found below:
		
	[_Ray Tracing in One Weekend_](https://raytracing.github.io/books/RayTracingInOneWeekend.html)
	
	[_Ray Tracing: The Next Week_](https://raytracing.github.io/books/RayTracingTheNextWeek.html)
	
	This is my most recent activity as of writing. I'm not a particularly artistic person, nor do I enjoy constructing scenes
	to look interesting. So I followed the ray tracing courses closely, and did not significantly modify the scenes.
	Here are a few piectures of the renders:
	
	![Final Render of "In a Weekend"](images/RT1WE_final.png)![Perlin Noise](images/perlin.png)![Simple Lights](images/simpleLight.png)
	
	![Cornell Box](images/cornellBox.png)
	
	My code does look slightly different from the version available on their github. 
	For one, i use stb_image to write the resulting image to a png file. 
	Another change I made is to use templates for vectors and other classes. But I quickly stopped using them,
	as I realized I would never use this code again and I'm writing code that will simply never be used.
	
	The final render of Ray Tracing: The Next Week is being rendered at the moment, so that'll have to wait.
	
	(##) **What's Next?**
	
	As I stand now, I am at a crossroads, with the potential to do anything I want. I know that I would like to create my own game engine. 
	But I don't feel equipped to make it yet. But truthfully, how often do you feel prepared when embarking on a difficult task?
	This choice paralysis is stopping me dead in my tracks, as I am unsure of myself and the steps I need to take to continue my Graphics Programming journey.
	I will end this journal entry with a set of points that I wish to complete over the coming months:
	
	- Learn Vulkan
	- Read [Game Engine Architecture](https://www.gameenginebook.com/)
	- Write my own Game Engine
	- Do Ray Tracing in Real-Time
	
	As for my professional plans in the near future, I will soon be moving to Canada to take a 
	[Graduate Certificate in Game Programming](https://www.sheridancollege.ca/programs/game-development-advanced-programming).
	I'm taking this brief time to study what I personally find interesting to prepare for my future after the course.
	
	[Edit 2024/05/01] 
	Less than 24 hours after writing this post, I received an email saying that the Graduate Certificate program has been suspended for my intake.
	To say that this news is devastating is an understatement. We will have to see how this situation plays out, but this may derail my long-term plans.
 
<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>